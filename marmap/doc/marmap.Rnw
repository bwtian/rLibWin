\documentclass[a4paper]{article}
\usepackage{cite}
\usepackage[pdftex,bookmarks=true,colorlinks=true,linkcolor=blue,citecolor=blue,breaklinks]{hyperref}
\bibliographystyle{plos2009}

\SweaveOpts{keep.source=TRUE}

% \VignetteIndexEntry{marmap}

\title{Making and using bathymetric maps in R with marmap}
\author{Eric Pante \& Benoit Simon Bouhet}
\begin{document}
\maketitle

\begin{center}
\line(1,0){250}
\end{center}

\tableofcontents

<<fig=FALSE,echo=FALSE>>=
options(width=60,continue="  ")
options(SweaveHooks=list(fig=function()
              par(mar=c(5.1, 4.1, 1.1, 2.1))))
library(marmap)
marmap::read.bathy('png.xyz', header=F, sep="\t") -> papoue
@

\begin{center}
\line(1,0){250}
\end{center}

\section{Introduction}

In this vignette we introduce \verb"marmap", a package designed for manipulating bathymetric data in R. \verb"marmap" uses simple latitude-longitude-depth data in ascii format and takes advantage of the advanced plotting tools available in R to build publication-quality bathymetric maps. Functions to query data (bathymetry, sampling information...) directly by clicking on \verb"marmap" maps are available. Bathymetric and topographic data can also be used to constrain the calculation of realistic shortest path distances. Such information can be used in molecular ecology, for example, to evaluate genetic isolation by distance in a spatially-explicit framework.

\section{A quick tutorial}

In this section, we will produce bathymetric maps of Papua New Guinea, Hawaii and the NW Atlantic. 

\subsection{Getting data into R}

Launch R. Navigate to your work folder (for example, with \verb"setwd()"). Then launch the marmap  package.  The simplest way to get bathymetric data into R for use with \verb"marmap" is to use the \verb"getNOAA.bathy()" function. It queries the ETOPO1 dataset (Armante and Eakins 2009) hosted on the NOAA server, based on coordinates and a resolution given by the user (please note that this function depends on the availability of the NOAA server!). In one line, we can get the data into R and start plotting: 

<<eval=FALSE,echo=TRUE>>=
library(marmap)
getNOAA.bathy(lon1 = 140, lon2 = 155, lat1 = -13, lat2 = 0, 
	resolution = 10) -> papoue
summary(papoue)
@

<<echo=FALSE>>=
summary(papoue)
@

\verb"summary.bathy()" helps you check the data ; because bathy is a class, and R an object-oriented language, you just have to use \verb"summary()".  R will recognize that you are feeding \verb"summary()" an object of class bathy. This is also true for \verb"plot.bathy"  and \verb"plot()".

\subsection{Plotting bathymetric data}

We can now use \verb"plot.bathy()" (or \verb"plot()", because R will recognize the object is of class bathy) to map the data. You can see that the 10 minute resolution is a bit rough, but enough to demonstrate how \verb"marmap" works (to increase the resolution, simply change the value for the \verb"resolution" argument to a smaller value).

<<fig=TRUE,echo=TRUE>>=
plot(papoue)
@
 
We can now use some of the options of \verb"plot.bathy()" to make the map more informative. First, we can plot a heat map, using the built in color palette. We can also add a scale in kilometers. 

<<fig=TRUE,echo=TRUE>>=
plot(papoue, image = TRUE)
scaleBathy(papoue, deg = 2, x = "bottomleft", inset = 5)
@

The \verb"bpal" options allows you to use a custom color palette, which can be easily prepared with the R function \verb"colorRampPalette()". We store the color ramp in the object called \verb"blues", and when we call it in \verb"plot.bathy()", we specify how many colors need to be used in the palette (here 100). 

<<fig=TRUE,echo=TRUE>>=
colorRampPalette(c("red","purple","blue","cadetblue1",
	"white")) -> blues
plot(papoue, image = TRUE, bpal = blues(100))
@

For maps using the \verb"image" option of \verb"plot.bathy()", you might see that the PDF rendering of your map is slightly different from the way it looks in R: the small space between cells becomes visible. This is probably due to the way your system handles PDFs. A simple way around this phenomenon is to export the map in a raster (rather than vector) format. You can use the \verb"tiff()", \verb"jpeg()", \verb"bmp()" or \verb"png()" functions available in R. This map looks a little crowded ; let's dim the isobaths (dark grey color and lighter line width), and strengthen the coastline (black color and thicker line width). The deepest isobaths will be hard to see on a dark blue background ; we can therefore choose to plot these in light grey to improve contrast. The option \verb"drawlabel" controls whether isobath labels (e.g. ``-3000'') are plotted or not. 

<<fig=TRUE,echo=TRUE>>=
plot(papoue, image = TRUE, bpal = blues(100),
	deep = c(-9000, -3000, 0), shallow = c(-3000, -10, 0),
	step = c(1000, 1000, 0), lwd = c(0.8, 0.8, 1),
	col = c("lightgrey", "darkgrey", "black"),
	lty = c(1, 1, 1), drawlabel = c(FALSE, FALSE, FALSE))
@

\subsection{Using bathymetric data for further analysis}

We can use the \verb"get.transect()" and \verb"plotProfile()" functions to extract and plot a depth cross section from the \verb"papoue" dataset. \verb"get.transect()" will use the coordinates you input to calculate the coordinates and depths along your transect, and calculate the great circle distance separating each point along the transect from the point of origin (in kilometers). 

<<fig=FALSE,echo=TRUE>>=
get.transect(papoue, 151, -6, 153, -7, distance = TRUE)
@

We can plot that information on a map and make a cross section plot with \verb"plotProfile()". Again, the very low resolution of the dataset produces an analysis with little information. You can get transect information and make a cross-section plot by directly clicking on the map, using the \verb"locator" option of \verb"get.transect()".

\setkeys{Gin}{width=\textwidth}
<<fig=TRUE,echo=TRUE,width=8,height=4>>=
get.transect(papoue, 151, -6, 153, -7, 
	distance = TRUE) -> transect
plotProfile(transect)
@
\setkeys{Gin}{width=.8\textwidth}

We can also use \verb"get.depth()" to retrieve depth information by either clicking on the map or by providing a set of longitude/latitude pairs (see help pages). This is helpfull to get depth information along a GPS track record for instance. If the argument \verb;distance; is set to TRUE, the haversine distance (in km) from the first data point on will also be computed. The output will look like this: 
 
\begin{verbatim}
> get.depth(papoue, distance=TRUE)  
Waiting for interactive input: click any number of times on the map, then press 'Esc'
       Lon       Lat Depth  Dist.km
1 146.0200 -2.601702  -758   0.0000
2 147.6167 -1.844152  -583 196.3933
3 149.3193 -2.607345 -2121 366.4942
4 150.7295 -4.249027 -2289 553.8867
\end{verbatim}

\verb"get.sample()" can be used in combination with a table containing sampling information to retrieve sample information by clicking on the map. Let's make a fake table of sampling data and use it for plotting and use with \verb"get.sample()": 

<<fig=FALSE,echo=TRUE>>=
x = c(142.1390, 142.9593, 144.0466, 145.9141,
      145.9372, 146.0115, 145.9141, 146.8589,
      146.6651, 147.1772, 147.2856, 152.7475,
      152.5025, 152.7816, 152.9010, 153.2314)
y = c(-2.972065, -3.209449, -3.391399, -4.675720,
      -4.914153, -5.130116, -5.329641, -2.587792,
      -2.897221, -3.250368, -2.720080, -6.005769,
      -6.211152, -6.326915, -5.990206, -6.023344)

paste("station",1:16, sep = "") -> station
data.frame(x, y, station) -> sampling
@

We have now created a small table that we can use for further analysis. Let's plot them on a map:

<<fig=TRUE,echo=TRUE>>=
head(sampling) # a preview of the first 6 lines of the dataset. 

plot(papoue, image = TRUE, bpal = blues(100),
	deep = c(-9000, -3000, 0), shallow = c(-3000, -10, 0),
	step = c(1000, 1000, 0), lwd = c(0.8, 0.8, 1),
	col = c("lightgrey", "darkgrey", "black"),
	lty = c(1, 1, 1), drawlabel = c(FALSE, FALSE, FALSE))

# add points from the sampling.csv, and add text to the plot:
points(sampling$x, sampling$y, pch = 21, col = "black", 
	bg = "yellow", cex = 1.3)
text(152, -7.2, "New Britain\nTrench", col = "white", font = 3)
@

By clicking on the map, we can select the area in the New Britain Trench, to get information on the sampling stations of that area. \verb"get.sample()" will detect that there are samples in the area selected, and return the locations relative for these samples. 

 \begin{verbatim}
> # click twice on the map to delimit an area:
> get.sample(papoue, sampling, col.lon = 1, col.lat = 2)
          x         y   station
12 152.7475 -6.005769 station12
13 152.5025 -6.211152 station13
14 152.7816 -6.326915 station14
15 152.9010 -5.990206 station15
16 153.2314 -6.023344 station16
\end{verbatim}

We can use the depth data when plotting points: 

<<fig=TRUE,echo=TRUE>>=
# make a table of fake sampling information, with fake depth
samp.depth = sample(seq(-3000, -1000, by = 50), size = 16)
data.frame(sampling$x, sampling$y, samp.depth) -> sp
names(sp) <- c("lon", "lat", "depth")
head(sp)

# plot map
par(mai=c(1,1,1,1.5))
plot(papoue, deep = c(-4500, 0), shallow = c(-50, 0), step = c(500, 0), 
	lwd = c(0.3, 1), lty = c(1, 1), col = c("grey", "black"),
	drawlabels = c(FALSE, FALSE))	
scaleBathy(papoue, deg = 3, x = "bottomleft", inset = 5)

# set color palette	
max(-sp$depth, na.rm = TRUE) -> mx
colorRampPalette(c("white", "lightyellow", "lightgreen", 
	"blue", "lightblue1", "purple")) -> ramp
blues <- ramp(mx)

# plot points and color depth scale	
points(sp[,1:2], col = "black", bg = blues[-sp$depth], 
	pch = 21, cex = 1.5)
library(shape)
colorlegend(zlim = c(mx, 0), col = rev(blues), main = "depth (m)",
	posx = c(0.85, 0.88))
@

The function \verb"get.area()" can be used to calculate the projected surface area (the projecting surface being the ocean surface). For example, in the case of the Hawaiian Archipelago, we can calculate the surface area of the bathyal (1,000 to 4,000 m) and abyssal regions (4,000 to about 6,000 m). 

<<fig=FALSE,echo=TRUE>>=
data(hawaii)
get.area(hawaii, level.inf = -4000, level.sup = -1000) -> bathyal
get.area(hawaii, level.inf = min(hawaii), level.sup = -4000) -> abyssal
round(bathyal$Square.Km, 0) -> ba
round(abyssal$Square.Km, 0) -> ab
@ 

The function \verb"get.area()" returns a surface area in square kilometers (\verb"$Square.Km"), and a matrix of zeros and ones delimiting the area of interest. The \verb"$Lon", \verb"$Lat" and \verb"$Area" objects can be used to display these areas: 

<<fig=TRUE,echo=TRUE>>=
plot(hawaii, lwd = 0.2)
image(bathyal$Lon, bathyal$Lat, bathyal$Area, 
	col = c("transparent", rgb(0.7, 0, 0, 0.3)), add = TRUE)
image(abyssal$Lon, abyssal$Lat, abyssal$Area, 
	col = c("transparent", rgb(0.7, 0.7, 0.3, 0.3)), add = TRUE)
legend("bottomleft", 
	legend = c(paste("bathyal:", ba, "km2"), 
	paste("abyssal:", ab, "km2")), 
	fill = c(rgb(0.7, 0, 0, 0.3), rgb(0.7, 0.7, 0, 0.3)))
@

\subsection{Using bathymetric data for least-cost path analysis}

\verb"marmap" contains functions to facilitate least-cost path analysis that are based on the \verb"raster" and \verb"gdistance" packages (van Etten 2012a, 2012b). \verb"gdistance" calculates routes in a heterogeneous landscape, taking obstacles into account. These obstacles can be defined in \verb"marmap" based on bathymetric data. We will use the Hawaiian islands as our playground for this section.  

<<fig=FALSE,echo=TRUE>>=
data(hawaii, hawaii.sites)
sites <- hawaii.sites[-c(1,4),]
rownames(sites) <- 1:4
@ 

We first compute a transition to be used by \verb"lc.dist" to compute least cost distances between locations. The transition object generated by \verb"trans.mat" contains the probability of transition from one cell of a bathymetric grid to adjacent cells, and depends on user defined parameters. \verb"trans.mat" is especially usefull when least cost distances need to be calculated between several locations at sea. The default values for \verb"min.depth" and \verb"max.depth" ensure that the path computed by \verb"dist.geo" will be the shortest path possible at sea avoiding land masses. The path can be constrained to a given depth range by setting manually \verb"min.depth" and \verb"max.depth". For instance, it is possible to limit the possible paths to the continental shelf by setting \verb"max.depth=-200". Inaccuracies of the bathymetric data can occasionally result in paths crossing land masses. Setting \verb"min.depth" to low negative values (e.g. -10 meters) can limit this problem. 

\verb"trans1" is a transition object contained only by land masses. \verb"trans2" is a transition object that makes travel impossible in waters shallower than 200 meters depth. This step takes a little time. 

\begin{Schunk}
\begin{Sinput}
> trans1 <- trans.mat(hawaii)
> trans2 <- trans.mat(hawaii, min.depth = -200)
\end{Sinput}
\end{Schunk}

We can now use these transition objects to calculate least cost distances for \verb"trans1" and \verb"trans2". The output of \verb"lc.dist" is a list of geographic positions corresponding to the least-cost path. 

\begin{Schunk}
\begin{Sinput}
> out1 <- lc.dist(trans1, sites, res = "path")
\end{Sinput}
\begin{Soutput}
  |=================================================| 100%
\end{Soutput}
\begin{Sinput}
> out2 <- lc.dist(trans2, sites, res = "path")
\end{Sinput}
\begin{Soutput}
  |=================================================| 100%
\end{Soutput}
\end{Schunk}


We use the \verb"lapply" function to extract information from these lists and plot lines. Thick orange lines correspond to least-cost paths only constrained by landmasses Thin black lines are paths constrained by the 200 m isobath. We store the result of \verb"lapply" in a \verb"dummy" variable to avoid printing of unnecessary information. The coastline is in black, the 200 m isobath is in blue, and isobaths between 5000 and 200 m depth are in grey. Our sampling points are in blue.

\begin{Schunk}
\begin{Sinput}
> plot(hawaii, xlim = c(-161, -154), ylim = c(18, 23), 
          deep = c(-5000, -200, 0), shallow = c(-200, 0, 0), 
          col = c("grey", "blue", "black"), step = c(1000, 200, 1), 
          lty = c(1, 1, 1), lwd = c(0.6, 0.6, 1.2), 
          draw=c(FALSE, FALSE, FALSE))
> points(sites, pch = 21, col = "blue", bg = col2alpha("blue", .9),
          cex = 1.2)
> text(sites[,1], sites[,2], lab = rownames(sites),
          pos = c(3, 4, 1, 2), col = "blue")
> lapply(out1, lines, col = "orange", lwd = 5, lty = 1) -> dummy
> lapply(out2, lines, col = "black", lwd = 1, lty = 1) -> dummy
\end{Sinput}
\end{Schunk}
\includegraphics{lc.pdf}

The option \verb"res" of \verb"lc.dist" controls whether path coordinates or distances between points (in kilometers) are outputted.  Let's see how these different scenarios (no constraint: great-circle distance, dist0 ; avoid landmasses: dist1 ; avoid areas shallower than 200 m: dist2) effect distances between sampling points:

\begin{Schunk}
\begin{Sinput}
> library(fossil)
> dist0 <- round(earth.dist(sites), 0)
> dist1 <- lc.dist(trans1, sites, res = "dist")
> dist2 <- lc.dist(trans2, sites, res = "dist")
> dist0
\end{Sinput}
\begin{Soutput}
    1   2   3
2 226        
3 387 381    
4 355 517 331
\end{Soutput}
\begin{Sinput}
> dist1
\end{Sinput}
\begin{Soutput}
    1   2   3
2 230        
3 391 401    
4 365 529 334
\end{Soutput}
\begin{Sinput}
> dist2
\end{Sinput}
\begin{Soutput}
    1   2   3
2 230        
3 423 403    
4 365 533 334
\end{Soutput}
\end{Schunk}


Note: You can check out the help file for \verb"lc.dist" to see how we can combine these functions with cross-section calculations and plotting. 

\subsection{Landscape Genetics}

The distance objects created in the section above are formatted as matrices that can be used in R or exported to be used in GenePop (Rousset 2008), TESS (Durand et al 2009), or other software. As an example, these distances can be used to perform a Mantel test, as implemented in the package \verb"ade4" (\verb"mantel.rtest()" function ; Chessel and Dufour 2004, Dray et al 2007, Dray and Dufour 2007). The matrices produced in \verb"marmap" are ready for use with \verb"ade4". For export and use in external programs, the function \verb"write.matrix()" of the \verb"MASS" package (Venables and Ripley 2002) will be helpful. 

\subsection{3D plotting}

R contains tools to plot data in three dimensions. We can use the function \verb"wireframe()" of the package \verb"lattice" to make a 3D representation of the NW Atlantic and its seamount chains. \verb"wireframe()" is not part of \verb"marmap", and was therefore not meant to work with objects of class bathy. We need to use the function \verb"unclass()" to make our data available to  \verb"wireframe()". Make sure to adjust the \verb"aspect" option of \verb"wireframe()", to minimize vertical exaggeration and biased latitude / longitude aspect ratio. 

\begin{verbatim}
data(nw.atlantic)
atl <- as.bathy(nw.atlantic)
library(lattice)
wireframe(unclass(atl), shade = TRUE, aspect = c(1/2, 0.1))
\end{verbatim}
\includegraphics[width=\textwidth]{3d.pdf}

The \verb"marmap" function \verb"get.box()" can be coupled with the \verb"lattice" function \verb"wireframe" to produce 3D plots of belt transects of given width. Let's use the NW Atlantic data to investigate these functions, and look at the New England and Corner Rise seamount chains. 

<<fig=TRUE,echo=TRUE>>=
data(nw.atlantic) ; atl <- as.bathy(nw.atlantic)
plot(atl, xlim = c(-70, -52), 
	deep = c(-5000, 0), shallow = c(0, 0), step = c(1000, 0), 
	col = c("lightgrey", "black"), lwd = c(0.8, 1), 
	lty = c(1, 1), draw = c(FALSE, FALSE))
        
get.box(atl, x1 = -68.6, x2 = -53.7, y1 = 42.4, y2 = 32.5, 
	width = 3, col = "red") -> out
@
<<fig=TRUE,echo=TRUE>>=
library(lattice)             
wireframe(out, shade = TRUE, zoom = 1.1,
	aspect = c(1/4, 0.1), 
	screen = list(z = -60, x = -55), 
	par.settings = list(axis.line = list(col = "transparent")),
	par.box = c(col = rgb(0, 0, 0, 0.1)))
@

\subsection{Preparing maps in the Pacific antimeridian region}

The antimeridian (or antemeridian) is the 180th meridian and is located about in the middle of the Pacific Ocean, east of New Zealand and Fidji, west of Hawaii and Tonga. If you want to prepare a map of the Aleutian Islands (Alaska), your latitude values may, for example, go from 165 to 180 degrees East, and 180 to 165 degrees West. Crossing the antemeridian means that you will need to download data for the eastern (165 to 180) and the western (-180 to -165) portions of the area of interest (for example, GEBCO will tell you ``The Westernmost is more Easterly than the Easternmost. Please amend your search query'' if you try to download data for the Aleutians in one step). 
\verb;getNOAA(); has an argument to deal with the antemeridian region. For the Aleutians, you would use the \verb;antimeridian; argument. \verb;summary.bathy(); can interpret antimeridian areas as well. When you plot your antimeridian region, the default behavior of \verb;plot.bathy(); is to scale longitudes from 0 to 360 degrees (170E to 170W would be displayed as 170, 190 instead of 170, -170). You can use the argument \verb;axes=FALSE; in \verb;plot.bathy(); and add correct labels with \verb;antimeridian.box();. We have set the default behavior of \verb;plot.bathy(); in this way to remind the user that the scale of the bathy object, in the antimeridian region, goes from 0 to 360; if you need to plot points on the map, you need to take this into account (i.e. a point at -170 longitude must be plotted using 190, not 170 or -170). 

\begin{verbatim}
> getNOAA.bathy(165,-145,50,65, resolution=5,
  + antimeridian=TRUE) -> aleu
> summary(aleu)
> plot(aleu, image=TRUE, 
  + bpal=list(c(0,max(aleutians),grey(.7),grey(.9),grey(.95)),
  + c(min(aleutians),0,"darkblue","lightblue")),
  + land=TRUE,lwd=0.1,axes=FALSE)
> plot(aleutians,n=1,lwd=.8,add=T)
> antimeridian.box(aleu)
\end{verbatim}
\includegraphics[width=\textwidth]{aleu.pdf}

Alternatively, it is possible to import two compatible \verb;bathy; objects (for instance from GEBCO), one for the eastern part and one for the western part of the area of interest. The function \verb;collate.bathy; takes care of the stitching process: relabelling longitudes in the 0-360 degrees range, removing duplicated data (i.e. the data for longitude 180 is often present once in each individual dataset and thus needs to be removed once), etc. Providing that we downloaded two files ``east.nc'' and ``west.nc'' from the GEBCO website, creating a proper bathy object for the antimeridian region is as simple as:

\begin{verbatim}
> a <- getGEBCO.bathy("east.nc")
> b <- getGEBCO.bathy("west.nc")
> stitched <- collate.bathy(a,b)
\end{verbatim}

\section{Data import and export strategies in marmap}

\subsection{Overview of the different import and export strategies available in marmap}

\verb"getNOAA.bathy()" is the easiest way to load data into R, but it depends on the NOAA download protocol, and one must have an internet connection (see above). However, setting the \verb;keep; argument to \verb;TRUE; will save on disk the data downloaded from the NOAA servers when the function is called for the first time. Any subsequent call to \verb;getNOAA.bathy(); with the same list of arguments (i.e. same longitudes, latitudes and resolution) will preferentially load the dataset saved on disk in the current working directory. This allows the users to run scripts without having to query the NOAA servers and download the same data again and again, making the use of \verb;getNOAA.bathy(); possible even off-line. \verb"read.bathy()" allows import of data into R, and this data can be located on a drive ; an internet connection is therefore not mandatory. This is a good way to import data that have been saved locally on your drive, and may be faster than re-downloading data from the NOAA server at the beginning of each R session. If the user is building maps routinely, we propose two functions to create a local database that can be accessed from within R. These functions are \verb"setSQL()" and \verb"subsetSQL()". \\

\begin{table}
	\begin{center}
		\small
		\begin{tabular}{|l|p{2.2cm}|p{2.2cm}|p{2.2cm}|l|}
		\hline 
		{\bf Function} &  {\bf Job} & {\bf Input} & {\bf Output} & {\bf Internet} \\
		\hline
		\verb"getNOAA.bathy()" & downloads data from NOAA server & coordinates of bounding box and resolution & data matrix of class bathy &        yes \\
		\hline
		\verb"readGEBCO.bathy()" & imports data from GEBCO file & name of external file in netCDF format & data matrix of class bathy &        no \\
		\hline
		\verb"read.bathy()" & imports data into R & name of external file with xyz data & data matrix of class bathy &         no \\
		\hline
		\verb"setSQL()" & creates a local SQL database of bathymetric data & name of external file with xyz data & an SQL database &         no \\
		\hline
		\verb"subsetSQL()" & queries a local SQL database & coordinates of bounding box and resolution & data matrix of class bathy &         no \\
		\hline
		\verb"as.xyz()" & converts a dataset of class bathy into an xyz table & dataset of class bathy (an R object) & an xyz table (an R object) &         no \\
		\hline
		\verb"as.bathy()" & converts an xyz table into an dataset of class bathy & an xyz table (an R object) & dataset of class bathy (an R object) &         no \\
		\hline
		\end{tabular}
	\end{center}
\end{table}

\subsection{Importing bathymetric data from GEBCO: readGEBCO.bathy()}

\verb"readGEBCO.bathy()" provides a data source alternative to the NOAA-hosted ETOPO1 data. The GEBCO data, hosted on the British Oceanographic Data Center server, is available at the 30 second and 1 minute resolutions. Both types can be imported using \verb;readGEBCO.bathy();, using the \verb;ncdf; package to load netCDF data into R. The argument \verb;db; specifies whether data was downloaded from the 30 arcseconds database (GEBCO\_08) or the 1 arcminute database (GEBCO\_1min, the default). A third database type, GEBCO\_08 SID, is available from the website. This database contains a Source IDentifier (SID) specifying which grid cells have depth information based on soundings; it does not contain bathymetry or topography data. \verb;readGEBCO.bathy; can read this type of database with \verb;db = "GEBCO_08";, and only the SID information will be included in the object of class \verb;bathy;. Therefore, to display a map with both the bathymetry and the SID information, you will have to download both datasets from GEBCO, and import and plot both independently. Here is an example for the region of the Mediterranean Sea including Corsica and Sardinia:

\begin{Schunk}
\begin{Sinput}
> readGEBCO.bathy("gebco_08_7_38_10_43_corsica.nc", db="GEBCO_08") -> med 
> summary(med)     # the bathymetry data

> readGEBCO.bathy("gebco_SID_7_38_10_43_corsica.nc", db="GEBCO_08")-> sid
> summary(sid)     # the SID data

# a pretty custom color palette
> colorRampPalette(c("lightblue","cadetblue2","cadetblue1","white")) -> blues 

# a first plot for bathymetry
> plot(med, n=1, im=T, bpal=blues(100), 
      main="Corsica & Sardinia bathymetry\n GEODAS 08 & SID datasets") 
# a second layer with the SID data
> contour(as.numeric(rownames(sid)), as.numeric(colnames(sid)), sid, 
      drawlabels=F, lwd=.1, add=T)
\end{Sinput}
\end{Schunk}

The argument \verb;resolution; specifies the resolution of the object of class \verb;bathy;. Because the resolution of GEBCO data is rather fine, we offer the possibility of downsizing the dataset with \verb;resolution;. \verb;resolution; is in units of the selected database: in "GEBCO\_1min", \verb;resolution; is in minutes; in "GEBCO\_08", \verb;resolution; is in 30 arcseconds (that is, \verb;resolution = 3; corresponds to 3x30sec, or 1.5 arcminute).

\subsection{Getting bathymetric data from an xyz file: read.bathy()}

\verb"read.bathy()" will read xyz data from any source. Here, we will get ETOPO1 data hosted on the NOAA GEODAS server (NOAA National Geophysical Data Center 2013). To get the data, use the following link: \\

\verb"http://www.ngdc.noaa.gov/mgg/gdas/gd_designagrid.html" \\

To prepare data from NOAA, give a name to your custom grid, choose the database (ETOPO1 1-minute Global Relief), fill the custom grid form (upper latitude: 0, lower latitude: 13S, left longitude: 140E, right longitude: 155E) for a grid cell size of 10 minute, and choose "XYZ (lon,lat,depth)" as the "Output Grid Format", "No Header" as the "Output Grid Header", and either of the space, tab of comma as the column delimiter (either can be used, but "comma" is the default import format of \verb"read.bathy()"). Choose "omit empty grid cells" to reduce memory usage. Submit your job, and retrieved your data. You will get a zipped folder, in which you will find (in a subfolder) a .xyz  file with your data. Place it, for example, in your work folder. 

The resolution of 10 minutes is a low resolution that will keep the size of the example file small, about 200 kb. Increasing the resolution to 1 minute would result in a file size of about 20 mb. 

Launch R. Navigate to your work folder (for example, with \verb"setwd()"). Then launch the marmap  package.  and load your xyz data (we will call it ``png.xyz'') with \verb"read.bathy()". This converts your data into an R object of class ``bathy.'' \verb"summary.bathy()" helps you check the data ; because bathy is a class, and R an object-oriented language, you just have to use \verb"summary()", because R will recognize that you are feeding \verb"summary()" an object of class bathy. This is also true for \verb"plot.bathy"  and \verb"plot()".

<<fig=FALSE,echo=TRUE>>=
library(marmap)
read.bathy('png.xyz', header = FALSE, sep = "\t") -> papoue
summary(papoue)
@

The \verb;read.bathy; function can import bathymetric data for non rectangular areas as is often the case for custom datasets acquired by various types of sonar systems (e.g. Multibeam Echo Sounders). Please note however that depending on the size of the xyz file, the resolution of the data and the shape of the area covered by the data, the import can take up to sevral minutes.

\subsection{Getting bathymetric data from NOAA: local SQL data{\-}base}

\verb"setSQL()" and \verb"subsetSQL()" create and query a local SQL database for bathymetric data. These tools are made for routine use with no internet connection. The full ETOPO1 database, or a subset (for example), can be downloaded on your computer, and used to set an SQL database, which size will be approximately the same as your original xyz data (unzipped ETOPO1 is about 5 Go). The advantage of SQL, a language for querying large databases, are manyfold. Its use will allow rapid upload of data into R, directly as bathy objects (and therefore directly useable for plotting and analysis) with a smaller footprint on your memory than if you tried to load a very large xyz file into R and then subset-ed it. Here is a simple example on how to set up and use an SQL database for marmap. 

Use a local file with xyz data (we can re-use the png.xyz that we created above for use with \verb"read.bathy()"), and submit it to \verb"setSQL()". Make sure that no file called \verb"bathy_db" is present in your working directory. Also, make sure that the package \verb"RSQLite" (James and Falcon 2012) is installed and properly working. 

<<fig=FALSE,echo=TRUE>>=
require(RSQLite)
setSQL(bathy = "png.xyz", sep = "\t")
@ 

This will created a file \verb"bathy_db" in your directory, which size is about the size of (or larger than) your original data. If you want to create a database for frequent use, you just need to do this once. \verb"subsetSQL()" will know where to get the data in future R sessions. If \verb"setSQL()" worked properly, it will return \verb"TRUE". If there is a problem (e.g. database connection already open, database file already created ...) it will return \verb"FALSE".  Lets query a subset of the png dataset, and check that it is indeed what we asked for with the \verb"summary.bathy()" function:

<<fig=FALSE,echo=TRUE>>=
subsetSQL(min_lon = 145, max_lon = 150,
	 min_lat = -2, max_lat = 0) -> test
summary(test)
@

Finally, if you are done with the SQL dataset, you can remove it with

<<fig=FALSE,echo=TRUE>>=
system("rm bathy_db")
@

\section{Miscellaneous}
\subsection{Interactions with other packages}

\verb"marmap" interacts with multiple existing R packages for visualization and analysis, such as \verb"lattice" for building three-dimensional plots, and \verb"gdistance" for least-cost path calculations (see above). \verb"marmap" also contains functions to ease interactions with other packages dedicated to the analysis of spatial data. Data from class \verb"bathy" can be transformed into \verb"RasterLayer" objets for use in the \verb"raster" package \cite{Etten2012a} or into \verb"SpatialGridDataFrame" objects for use in the packages \verb"sp" \cite{Edzer2005,Bivand2008}. The full range of spatial analyses implemented in packages taking advantage of these classes are thus available for bathymetric data. The simple example presented below illustrate how to apply an arbitrary projection to \verb"bathy" objects using the function \verb"projectRaster" from the \verb"raster" package (n.b. a working installation of the \verb"rgdal" package is needed to use this function).

\begin{Schunk}
\begin{Sinput}
# Loads data of class bathy
> data(hawaii)

# Creates an object of class raster
> r1 <- as.raster(hawaii)

# Defines the target projection
> newproj <- "+proj=lcc +lat_1=48 +lat_2=33 +lon_0=-100 +ellps=WGS84"

# Creates a new projected raster object
> r2 <- projectRaster(r1,crs=newproj)

# Switches back to a bathy object
> hawaii.projected <- as.bathy(r2)

# Plots both the original and projected bathy objects
> plot(hawaii, image = TRUE, lwd = 0.3)
> plot(hawaii.projected, image = TRUE, lwd = 0.3, 
       xlab = "", ylab = "", axes = FALSE)
\end{Sinput}
\end{Schunk}
\includegraphics{hawaii1.pdf}\\

\includegraphics{hawaii2.pdf}


\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\providecommand{\urlprefix}{URL }

\bibitem{etopo1}
Amante C, Eakins BW (2009) Etopo1 1 arc-minute global relief model: Procedures,
  data sources and analysis.
\newblock NOAA Technical Memorandum NESDIS NGDC-24 : 1-19.

\bibitem{Bivand2008}
Bivand RS, Pebesma EJ, Gomez-Rubio V (2008) {Applied spatial data analysis with R}. 
\newblock Springer, NY. 

\bibitem{Chessel2004}
Chessel D, Dufour A, Thioulouse J (2004) {The ade4 package -I- One-table
  methods}.
\newblock R News 4: 5-10.

\bibitem{Dray2007b}
Dray S, Dufour A, Chessel D (2007) {The ade4 package-II: Two-table and K-table
  methods}.
\newblock R News 7: 47-52.

\bibitem{Dray2007}
Dray S, Dufour A (2007) {The ade4 package: implementing the duality diagram for
  ecologists}.
\newblock Journal of Statistical Software 22: 1-20.

\bibitem{Durand2009}
Durand E, Jay F, Gaggiotti O, Fran{\c c}ois O (2009) Spatial inference of
  admixture proportions and secondary contact zones.
\newblock Molecular Biology and Evolution 26: 1963-1973.

\bibitem{Etten2012a}
van Etten RJHJ (2012) {raster: Geographic data analysis and modeling}.
\newblock \urlprefix\url{http://CRAN.R-project.org/package=raster}.
\newblock R package version 2.0-41.

\bibitem{Etten2012b}
van Etten J (2012) {gdistance: Distances and routes on geographical grids}.
\newblock \urlprefix\url{http://CRAN.R-project.org/package=gdistance}.
\newblock R package version 1.1-4.

\bibitem{GEODAS}
{NOAA National Geophysical Data Center}.
\newblock {GEODAS Grid Translator - Design a grid}.
\newblock \urlprefix\url{http://www.ngdc.noaa.gov/mgg/gdas/gd\textunderscore
  designagrid.html}.

\bibitem{Edzer2005}
Pebesma EJ, Bivand RS (2005) {Classes and methods for spatial data in R}.
\newblock R News. 5:9-13.

\bibitem{RSQLite}
James DA, Falcon S (2012) {RSQLite: SQLite interface for R}.
\newblock \urlprefix\url{http://CRAN.R-project.org/package=RSQLite}.
\newblock R package version 0.11.2.

\bibitem{Rousset}
Rousset F, (2008) GENEPOP'007: a complete re-implementation of the genepop software for  Windows and Linux.
\newblock Molecular Ecology Resources 8: 103-106.

\bibitem{MASS}
Venables, W. N. and B. D. Ripley. 2002. Modern Applied Statistics with S. Fourth edition. Springer.

\end{thebibliography}


\end{document}
